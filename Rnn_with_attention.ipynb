{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rnn_with_attention.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3iPH8m_VRl7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous Area"
      ],
      "metadata": {
        "id": "snFg9H9r8b0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modifyLine(line):\n",
        "  line = line.replace(\"\\'ll\", \" will\")\n",
        "  line = line.replace(\"\\'re\", \" are\")\n",
        "  line = line.replace(\"\\'m\", \" am\")\n",
        "  line = line.replace(\"\\'d\", \" would\")\n",
        "  line = line.replace(\"n\\'t\", \" not\")\n",
        "  line = line.replace(\"\\'ve\", \" have\")\n",
        "  line = line.replace(\"\\'s\", \" \\'s\")\n",
        "  line = line.replace(\"s\\'\", \" \\'s\")\n",
        "\n",
        "  return line\n",
        "\n",
        "verb_forms_df = pd.read_csv(\"verbs.txt\")\n",
        "\n",
        "a = verb_forms_df[\"PresentTense\"].str.lower()\n",
        "b = verb_forms_df[\"PastTense\"].str.lower()\n",
        "c = verb_forms_df[\"PastParticiple\"].str.lower()\n",
        "d = verb_forms_df[\"PresentParticiple\"].str.lower()\n",
        "\n",
        "def ruin_grammer(line):\n",
        "  for i, word in enumerate(b):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  for i, word in enumerate(c):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  for i, word in enumerate(d):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  line = line.replace(\" a \", \" \")\n",
        "  line = line.replace(\" an \", \" \")\n",
        "  line = line.replace(\" from \", \" \")\n",
        "  line = line.replace(\" the \", \" \")\n",
        "  line = line.replace(\" on \", \" \")\n",
        "  line = line.replace(\" in \", \" \")\n",
        "  line = line.replace(\" at \", \" \")\n",
        "  line = line.replace(\" of \", \" \")\n",
        "  line = line.replace(\" is \", \" \")\n",
        "  line = line.replace(\" am \", \" \")\n",
        "  line = line.replace(\" to \", \" \")\n",
        "  line = line.replace(\" was \", \" \")\n",
        "  line = line.replace(\" were \", \" \")\n",
        "  line = line.replace(\" it \", \" \")\n",
        "  line = line.replace(\" your \", \" you \")\n",
        "      \n",
        "  return line"
      ],
      "metadata": {
        "id": "1X3IAY4krOhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup and data"
      ],
      "metadata": {
        "id": "pVK1KzVwoFxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A2mEJnPh5VS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import common_functions as cf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf.unzip_data(\"/content/drive/MyDrive/c415.zip\")\n",
        "X_c4 = []\n",
        "y_c4 = []\n",
        "x = True\n",
        "for i in range(25):\n",
        "  file1 = open(f'/content/{i}.csv', 'r')\n",
        "  for line in file1.readlines():\n",
        "    if line.count(\"value:\") >0:\n",
        "      line = line.replace(\": \\\"\", \": \")\n",
        "      line = line.replace(\".\\\"\\n\", \".\")\n",
        "      line = line.replace(\"?\\\"\\n\", \"?\")\n",
        "      line = line.replace(\"!\\\"\\n\", \"!\")\n",
        "      line = line.replace(\"\\\\'s\",\" is\")\n",
        "      line = line.replace(\"\\\\'re\",\" are\")\n",
        "\n",
        "      if x:\n",
        "        line = line.split(\"value: \")[1]\n",
        "        line = line.replace(\" of \",\" \")\n",
        "        line = line.replace(\" am \",\" \")\n",
        "        line = line.replace(\" are \",\" \")\n",
        "        line = line.replace(\" to \",\" \")\n",
        "        line = line.replace(\" be \",\" \")\n",
        "        line = line.replace(\" from \",\"\")\n",
        "        line = modifyLine(\"<start> \"+line+\" <end>\")\n",
        "        line = ruin_grammer(line)\n",
        "\n",
        "\n",
        "        X_c4.append(line)\n",
        "\n",
        "        x = False\n",
        "      else:\n",
        "        y_c4.append(\"<start> \"+line.split(\"value: \")[1]+\" <end>\")\n",
        "        x = True \n",
        "\n",
        "len(X_c4), len(y_c4)\n",
        "X_c4[-10:]\n",
        "y_c4[-10:]\n",
        "X_c4_selected = []\n",
        "y_c4_selected= []\n",
        "for i, line in enumerate(X_c4):\n",
        "  if not line.count(\"\\\\\")>0:\n",
        "    X_c4_selected.append(line)\n",
        "    y_c4_selected.append(y_c4[i])"
      ],
      "metadata": {
        "id": "rvsjgZS1iLvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_df1 = pd.read_csv(\"input1.csv\")\n",
        "X_df2 = pd.read_csv(\"input2.csv\")\n",
        "\n",
        "y_df1 = pd.read_csv(\"target1.csv\")\n",
        "y_df2 = pd.read_csv(\"target2.csv\")\n",
        "\n",
        "X_and_y_df3 = pd.read_csv(\"inputAndTarget3.txt\", header=None)\n",
        "\n",
        "X1 = X_df1[\"sentence\"].to_numpy()\n",
        "X2 = X_df2[\" sentence\"].to_numpy()\n",
        "X3 = X_and_y_df3[1].to_numpy()\n",
        "\n",
        "y1 = y_df1[\"sentence\"].to_numpy()\n",
        "y2 = y_df2[\"sentence\"].to_numpy()\n",
        "y3 = X_and_y_df3[1].to_numpy()\n",
        "\n",
        "split_size1 = int( len(X1) * .85 )\n",
        "split_size2 = int( len(X2) * .85 )\n",
        "split_size3 = int( len(X3) * .80 )\n",
        "\n",
        "X1_train, X1_val = X1[:split_size1], X1[split_size1:]\n",
        "X2_train, X2_val = X2[:split_size2], X2[split_size2:]\n",
        "X3_train, X3_val = X3[:split_size3], X3[split_size3:]\n",
        "\n",
        "y1_train, y1_val = y1[:split_size1], y1[split_size1:]\n",
        "y2_train, y2_val = y2[:split_size2], y2[split_size2:]\n",
        "y3_train, y3_val = y3[:split_size3], y3[split_size3:]\n",
        "\n",
        "X1_se = []\n",
        "X2_se = []\n",
        "X3_se = []\n",
        "y1_se = []\n",
        "y2_se = [] \n",
        "y3_se = []\n",
        "\n",
        "for line in X1:\n",
        "  X1_se.append(\"<start> \" +line + \" <end>\")\n",
        "for line in X2:\n",
        "  X2_se.append(\"<start> \" +line + \" <end>\")\n",
        "for line in X3:\n",
        "  X3_se.append(\"<start> \" +line + \" <end>\")  \n",
        "\n",
        "for line in y1:\n",
        "  y1_se.append(\"<start> \" +line + \" <end>\")\n",
        "for line in y2:\n",
        "  y2_se.append(\"<start> \" +line + \" <end>\")\n",
        "for line in y3:\n",
        "  y3_se.append(\"<start> \" +line + \" <end>\")    \n",
        "\n",
        "train_sentences = np.concatenate((X1_se, X3_se, X_c4_selected[:300000] )) \n",
        "train_targets = np.concatenate((y1_se, y3_se, y_c4_selected[:300000]))\n",
        "\n",
        "val_sentences = np.concatenate((X1_val, X3_val))\n",
        "val_targets = np.concatenate((y1_val, y3_val))"
      ],
      "metadata": {
        "id": "3uISUx6riURo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(inputs):\n",
        "  inputs = tf.strings.lower(inputs)\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'ll\", \" will\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'re\", \" are\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'m\", \" am\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'d\", \" would\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"n\\'t\", \" not\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'ve\", \" have\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"\\'s\", \" \\'s\")\n",
        "  inputs = tf.strings.regex_replace(inputs,\"s\\'\", \" \\'s\")  \n",
        "  return tf.strings.regex_replace(inputs,\n",
        "                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n",
        "\n",
        "max_length = 128\n",
        "\n",
        "vocabulary_size = 7500\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    output_sequence_length=max_length)\n",
        "\n",
        "s = []\n",
        "with open('/content/for_vocab.txt') as f:\n",
        "  for line in f.readlines():\n",
        "    line = line.replace(\"1\",\"\")\n",
        "    line = line.replace(\"2\",\"\")\n",
        "    line = line.replace(\"3\",\"\")\n",
        "    line = line.replace(\"4\",\"\")\n",
        "    line = line.replace(\"5\",\"\")\n",
        "    line = line.replace(\"6\",\"\")\n",
        "    line = line.replace(\"7\",\"\")\n",
        "    line = line.replace(\"8\",\"\")\n",
        "    line = line.replace(\"9\",\"\")\n",
        "    line = line.replace(\"0\",\"\")\n",
        " \n",
        "    s.append(line.replace(\",\", \" \"))\n",
        "train_sentences\n",
        "to_learn_vocabulary = np.concatenate((s, train_targets[:10000]))\n",
        "tokenizer.get_vocabulary()[:1000]\n",
        "tokenizer.adapt(to_learn_vocabulary)\n",
        "\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "FN3NJYxTiYBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE= 128\n",
        "\n",
        "def filter_max_tokens(inp, tar):\n",
        "  num_tokens = tf.maximum(tf.shape(inp)[1],tf.shape(tar)[1])\n",
        "  return num_tokens <=MAX_TOKENS\n",
        "\n",
        "def tokenize_pairs(inp, tar): \n",
        "  inp = tokenizer(inp)\n",
        "  tar = tokenizer(tar)\n",
        "  return inp, tar\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_sentences, train_targets))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_sentences, val_targets))\n",
        "\n",
        "train_ds = make_batches(train_ds)\n",
        "val_ds = make_batches(val_ds)"
      ],
      "metadata": {
        "id": "fB_A6LNuiXJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6IhHoTq9oDiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 512\n",
        "vocab_size = tokenizer.vocabulary_size()\n",
        "units = 256\n",
        "num_steps = len(train_sentences) // BATCH_SIZE\n",
        "features_shape = 8\n",
        "attention_features_shape = 128"
      ],
      "metadata": {
        "id": "NPk0TfoRqx0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, MAX_TOKENS, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, MAX_TOKENS, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, MAX_TOKENS, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, MAX_TOKENS, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "ewAcLDeFoJS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,):\n",
        "        super(Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UK1TQt5WrTPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru1 = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.gru2 = tf.keras.layers.GRU(int(self.units),\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')    \n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc3 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru1(x)\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "    x = self.fc2(x)\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "Z5iFjbkKtCDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim)\n",
        "decoder = Decoder(embedding_dim, units, vocab_size)"
      ],
      "metadata": {
        "id": "MuM9cBSKruGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "toXQZ9gct2XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "RHXMUev4t3dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"content/drive/MyDrive/rnn_checkpoints2/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n",
        "loss_plot = []"
      ],
      "metadata": {
        "id": "BJpR2970t-_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(inp)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "metadata": {
        "id": "XkOdde0JuqY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual training"
      ],
      "metadata": {
        "id": "FXdJDYc3_m1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 6\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, target)) in enumerate(train_ds):\n",
        "        batch_loss, t_loss = train_step(inp, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "metadata": {
        "id": "lVDfICHburqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting output"
      ],
      "metadata": {
        "id": "pO_4VWJ70TLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    features = encoder(tokenizer(sentence))\n",
        "\n",
        "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
        "        result.append(predicted_word)\n",
        "\n",
        "        if predicted_word == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result"
      ],
      "metadata": {
        "id": "e-uEFuV6z1pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate([\"I name mayank\"]))"
      ],
      "metadata": {
        "id": "a_Yj9xtNs235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ab24fe-9b8f-430e-dcb5-8f04c29ea571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['name', '[UNK]', '<end>'], array([[0.41988662, 0.22259225, 0.04262269, ..., 0.00251919, 0.00251919,\n",
            "        0.00251919],\n",
            "       [0.06450473, 0.06692412, 0.02803279, ..., 0.00672431, 0.00672431,\n",
            "        0.00672431],\n",
            "       [0.03329823, 0.05236207, 0.05605925, ..., 0.00686624, 0.00686624,\n",
            "        0.00686624],\n",
            "       ...,\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ],\n",
            "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
            "        0.        ]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving weights"
      ],
      "metadata": {
        "id": "3BnYreUvMrhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.save_weights(\"/content/drive/MyDrive/enc_weights2\")\n",
        "decoder.save_weights(\"/content/drive/MyDrive/dec_weights2\")"
      ],
      "metadata": {
        "id": "XnCtwdp1MtXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_weights(\"/content/drive/MyDrive/enc_weights2\")\n",
        "decoder.load_weights(\"/content/drive/MyDrive/dec_weights2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NWUHEJjZ7L1",
        "outputId": "12bcf01b-d809-4818-bff7-b8e0a90a30d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd5b46c7590>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}