{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Take 3 just combine context with input and something to indicate speration like end start token in between"
      ],
      "metadata": {
        "id": "EanfeQyCnoaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take 4 remove the concept of feeding the output all together just give input in encoder and context in decoder (all outputs/preds come at once)"
      ],
      "metadata": {
        "id": "8BngXv9Oo5Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take 5 diffrent embedding layer for context"
      ],
      "metadata": {
        "id": "EcqF7MeCSBZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take 6 pos tagging and ner "
      ],
      "metadata": {
        "id": "njYjlSXE8UxB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpmupdURcW-7"
      },
      "source": [
        "# Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG0TQ9b8fZp8",
        "outputId": "a6b27d08-3457-4d6d-bb95-1b4f1856b841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7ELjFh5cEcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866acbff-6008-4a04-ab3c-bdb5d1d26d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.19.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.28.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.6)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.8.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow_gpu-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-62.6.0-py3-none-any.whl (1.2 MB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Using cached h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Collecting packaging\n",
            "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting six>=1.12.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Using cached wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Using cached grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting numpy>=1.20\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Using cached absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Using cached google_auth-2.8.0-py2.py3-none-any.whl (164 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Using cached requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-gpu\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.4.8\n",
            "    Uninstalling pyasn1-0.4.8:\n",
            "      Successfully uninstalled pyasn1-0.4.8\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.6.15\n",
            "    Uninstalling certifi-2022.6.15:\n",
            "      Successfully uninstalled certifi-2022.6.15\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.0\n",
            "    Uninstalling zipp-3.8.0:\n",
            "      Successfully uninstalled zipp-3.8.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.0\n",
            "    Uninstalling requests-2.28.0:\n",
            "      Successfully uninstalled requests-2.28.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1-modules 0.2.8\n",
            "    Uninstalling pyasn1-modules-0.2.8:\n",
            "      Successfully uninstalled pyasn1-modules-0.2.8\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.0\n",
            "    Uninstalling oauthlib-3.2.0:\n",
            "      Successfully uninstalled oauthlib-3.2.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.0\n",
            "    Uninstalling cachetools-5.2.0:\n",
            "      Successfully uninstalled cachetools-5.2.0\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.12.0\n",
            "    Uninstalling importlib-metadata-4.12.0:\n",
            "      Successfully uninstalled importlib-metadata-4.12.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.8.0\n",
            "    Uninstalling google-auth-2.8.0:\n",
            "      Successfully uninstalled google-auth-2.8.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.1\n",
            "    Uninstalling wheel-0.37.1:\n",
            "      Successfully uninstalled wheel-0.37.1\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.1.2\n",
            "    Uninstalling Werkzeug-2.1.2:\n",
            "      Successfully uninstalled Werkzeug-2.1.2\n",
            "  Attempting uninstall: tensorboard-plugin-wit\n",
            "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
            "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
            "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 62.6.0\n",
            "    Uninstalling setuptools-62.6.0:\n",
            "      Successfully uninstalled setuptools-62.6.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.4\n",
            "    Uninstalling protobuf-3.19.4:\n",
            "      Successfully uninstalled protobuf-3.19.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.7\n",
            "    Uninstalling Markdown-3.3.7:\n",
            "      Successfully uninstalled Markdown-3.3.7\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.1.0\n",
            "    Uninstalling absl-py-1.1.0:\n",
            "      Successfully uninstalled absl-py-1.1.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 14.0.1\n",
            "    Uninstalling libclang-14.0.1:\n",
            "      Successfully uninstalled libclang-14.0.1\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.7.0\n",
            "    Uninstalling h5py-3.7.0:\n",
            "      Successfully uninstalled h5py-3.7.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: tensorflow-gpu\n",
            "    Found existing installation: tensorflow-gpu 2.9.1\n",
            "    Uninstalling tensorflow-gpu-2.9.1:\n",
            "      Successfully uninstalled tensorflow-gpu-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.2.0 which is incompatible.\n",
            "spacy 3.3.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.8.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.0.12 flatbuffers-1.12 gast-0.4.0 google-auth-2.8.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.47.0 h5py-3.7.0 idna-3.3 importlib-metadata-4.12.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 numpy-1.21.6 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.0 requests-oauthlib-1.3.1 rsa-4.8 setuptools-62.6.0 six-1.16.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.1 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "astunparse",
                  "certifi",
                  "charset_normalizer",
                  "flatbuffers",
                  "gast",
                  "google",
                  "h5py",
                  "idna",
                  "keras",
                  "numpy",
                  "opt_einsum",
                  "packaging",
                  "pkg_resources",
                  "pyasn1",
                  "pyasn1_modules",
                  "requests",
                  "six",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator",
                  "termcolor",
                  "urllib3",
                  "wrapt",
                  "zipp"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (62.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.47.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.26.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.28.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.1.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_datasets\n",
        "!pip3 install tensorflow-gpu --upgrade --force-reinstall\n",
        "!pip install tensorflow_text\n",
        "!pip3 install --quiet tensorflow-text\n",
        "\n",
        "import logging \n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import common_functions as cf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import tensorflow_hub as hub\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misc Area\n"
      ],
      "metadata": {
        "id": "k9IMXTydglzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modifyLine(line):\n",
        "  line = line.replace(\"\\'ll\", \" will\")\n",
        "  line = line.replace(\"\\'re\", \" are\")\n",
        "  line = line.replace(\"\\'m\", \" am\")\n",
        "  line = line.replace(\"\\'d\", \" would\")\n",
        "  line = line.replace(\"n\\'t\", \" not\")\n",
        "  line = line.replace(\"\\'ve\", \" have\")\n",
        "  line = line.replace(\"\\'s\", \" \\'s\")\n",
        "  line = line.replace(\"s\\'\", \" \\'s\")\n",
        "\n",
        "  return line\n",
        "\n",
        "#convert multiple continous dots into three continous dots\n",
        "\n",
        "file = open('/content/well.txt', 'r')\n",
        "allLines = file.readlines()\n",
        "string = \"\"\n",
        "for line in allLines:  \n",
        "  string += line\n",
        "string = string.replace(\"\\n\", \"\")\n",
        "prepared_lines = []\n",
        "temp_string = string[0]\n",
        "for i in range(1,len(string)):\n",
        "  if i< len(string)-2:\n",
        "    if (string[i] =='.' or string[i]==\":\") and ( (string[i+1].isspace() and (not string[i+2].islower())) or (string[i-1].islower() and (string[i+1].isupper() or string[i+1]==\"\\\"\"))):\n",
        "      temp_string += string[i]\n",
        "      prepared_lines.append(modifyLine(temp_string))\n",
        "      temp_string = \"\"\n",
        "    else:\n",
        "      temp_string += string[i]   \n",
        "    if string[i] =='.' and string[i+1] =='.' and string[i+2] =='.':  \n",
        "      temp_string += string[i]\n",
        "      prepared_lines.append(modifyLine(temp_string))\n",
        "      temp_string = \"\"\n",
        "      i=i+2\n",
        "  else:\n",
        "    if string[i] =='.':\n",
        "      temp_string += string[i]\n",
        "      prepared_lines.append(modifyLine(temp_string))\n",
        "      temp_string = \"\"\n",
        "    else:\n",
        "      temp_string += string[i]\n",
        "\n",
        "verb_forms_df = pd.read_csv(\"verbs.txt\")\n",
        "\n",
        "a = verb_forms_df[\"PresentTense\"].str.lower()\n",
        "b = verb_forms_df[\"PastTense\"].str.lower()\n",
        "c = verb_forms_df[\"PastParticiple\"].str.lower()\n",
        "d = verb_forms_df[\"PresentParticiple\"].str.lower()\n",
        "\n",
        "def ruin_grammer(line):\n",
        "  for i, word in enumerate(b):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  for i, word in enumerate(c):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  for i, word in enumerate(d):\n",
        "    line = line.replace(\" \"+word+\" \",\" \"+a[i]+\" \")\n",
        "  line = line.replace(\" a \", \" \")\n",
        "  line = line.replace(\" an \", \" \")\n",
        "  line = line.replace(\" from \", \" \")\n",
        "  line = line.replace(\" the \", \" \")\n",
        "  line = line.replace(\" on \", \" \")\n",
        "  line = line.replace(\" in \", \" \")\n",
        "  line = line.replace(\" at \", \" \")\n",
        "  line = line.replace(\" of \", \" \")\n",
        "  line = line.replace(\" is \", \" \")\n",
        "  line = line.replace(\" am \", \" \")\n",
        "  line = line.replace(\" to \", \" \")\n",
        "  line = line.replace(\" was \", \" \")\n",
        "  line = line.replace(\" were \", \" \")\n",
        "  line = line.replace(\" it \", \" \")\n",
        "  line = line.replace(\" your \", \" you \")\n",
        "  line = line.replace(\" and \", \" \")\n",
        "  line = line.replace(\" or \", \" \")\n",
        "      \n",
        "  return line"
      ],
      "metadata": {
        "id": "7RvCvSgzglj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming prepared lines are modified and standardized \n",
        "\n",
        "max_words = 128\n",
        "lengths = []\n",
        "\n",
        "for i in prepared_lines:\n",
        "  lengths.append(len(i.split()))\n",
        "\n",
        "train_context_misc=[] \n",
        "train_input_misc=[] \n",
        "train_target_misc = []\n",
        "\n",
        "context = \"\"\n",
        "for i, line in enumerate(prepared_lines):\n",
        "  temp_context = \"\"\n",
        "  train_context_misc.append(context)\n",
        "  train_input_misc.append(context+\"[END] [START]\"+ruin_grammer(line))\n",
        "  overflow = 2 + len(context.split()) + len(line.split()) - max_words\n",
        "  if overflow<0:\n",
        "    overflow = 0\n",
        "  splits = context.split()\n",
        "  for index in range(overflow, len(splits)):\n",
        "    temp_context += splits[index]+\" \"\n",
        "  context = temp_context + line\n",
        "  train_target_misc.append(line)"
      ],
      "metadata": {
        "id": "CAnBG1KPit5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hGwYOEHcyxB"
      },
      "source": [
        "# Text tokenization and detokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYVz1ILdc5sG"
      },
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")\n",
        "\n",
        "tokenizer = tf.saved_model.load(model_name).en\n",
        "MAX_TOKENS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se6aX8bheyJ5"
      },
      "source": [
        "# Getting data and prepraing Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_c= open(r'/content/drive/MyDrive/train_context', 'r')\n",
        "train_context_book = f_c.readlines()\n",
        "\n",
        "f_i= open(r'/content/drive/MyDrive/train_input', 'r')\n",
        "train_input_book = f_i.readlines()\n",
        "\n",
        "f_t= open(r'/content/drive/MyDrive/train_target', 'r')\n",
        "train_target_book = f_t.readlines()\n",
        "\n",
        "for i in range(len(train_context_book)):\n",
        "  train_context_book[i] = train_context_book[i].replace(\"\\n\",\"\")\n",
        "  train_input_book[i] = train_input_book[i].replace(\"\\n\",\"\")\n",
        "  train_target_book[i] = train_target_book[i].replace(\"\\n\",\"\")\n",
        "\n",
        "f_c.close()\n",
        "f_i.close()\n",
        "f_t.close()"
      ],
      "metadata": {
        "id": "dPGBhOZhOvhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cf.unzip_data(\"/content/drive/MyDrive/c415.zip\")\n",
        "#X_c4 = []\n",
        "#y_c4 = []\n",
        "#x = True\n",
        "#for i in range(25):\n",
        "#  file1 = open(f'/content/{i}.csv', 'r')\n",
        "#  for line in file1.readlines():\n",
        "#    if line.count(\"value:\") >0:\n",
        "#      line = line.replace(\": \\\"\", \": \")\n",
        "#      line = line.replace(\".\\\"\\n\", \".\")\n",
        "#      line = line.replace(\"?\\\"\\n\", \"?\")\n",
        "#      line = line.replace(\"!\\\"\\n\", \"!\")\n",
        "#      line = line.replace(\"\\\\'s\",\" is\")\n",
        "#      line = line.replace(\"\\\\'re\",\" are\")\n",
        "#\n",
        "#      if x:\n",
        "#        line = line.split(\"value: \")[1]\n",
        "#        line = line.replace(\" of \",\" \")\n",
        "#        line = line.replace(\" am \",\" \")\n",
        "#        line = line.replace(\" are \",\" \")\n",
        "#        line = line.replace(\" to \",\" \")\n",
        "#        line = line.replace(\" be \",\" \")\n",
        "#        line = line.replace(\" from \",\"\")\n",
        "#        line = modifyLine(line)\n",
        "#        #line = ruin_grammer(line)\n",
        "#\n",
        "#\n",
        "#        X_c4.append(line)\n",
        "#\n",
        "#        x = False\n",
        "#      else:\n",
        "#        y_c4.append(line.split(\"value: \")[1])\n",
        "#        x = True \n",
        "#\n",
        "#len(X_c4), len(y_c4)\n",
        "#X_c4[-10:]\n",
        "#y_c4[-10:]\n",
        "#X_c4_selected = []\n",
        "#y_c4_selected= []\n",
        "#for i, line in enumerate(X_c4):\n",
        "#  if not line.count(\"\\\\\")>0:\n",
        "#    X_c4_selected.append(line)\n",
        "#    y_c4_selected.append(y_c4[i])"
      ],
      "metadata": {
        "id": "wXd6yUxrKGWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_df1 = pd.read_csv(\"input1.csv\")\n",
        "#X_df2 = pd.read_csv(\"input2.csv\")\n",
        "\n",
        "#y_df1 = pd.read_csv(\"target1.csv\")\n",
        "#y_df2 = pd.read_csv(\"target2.csv\")\n",
        "#\n",
        "#X_and_y_df3 = pd.read_csv(\"inputAndTarget3.txt\", header=None)\n",
        "#\n",
        "#X1 = X_df1[\"sentence\"].to_numpy()\n",
        "#X2 = X_df2[\" sentence\"].to_numpy()\n",
        "#X3 = X_and_y_df3[1].to_numpy()\n",
        "#\n",
        "#y1 = y_df1[\"sentence\"].to_numpy()\n",
        "#y2 = y_df2[\"sentence\"].to_numpy()\n",
        "#y3 = X_and_y_df3[1].to_numpy()\n",
        "#\n",
        "#split_size1 = int( len(X1) * .85 )\n",
        "#split_size2 = int( len(X2) * .85 )\n",
        "#split_size3 = int( len(X3) * .80 )\n",
        "#\n",
        "#X1_train, X1_val = X1[:split_size1], X1[split_size1:]\n",
        "#X2_train, X2_val = X2[:split_size2], X2[split_size2:]\n",
        "#X3_train, X3_val = X3[:split_size3], X3[split_size3:]\n",
        "#\n",
        "#y1_train, y1_val = y1[:split_size1], y1[split_size1:]\n",
        "#y2_train, y2_val = y2[:split_size2], y2[split_size2:]\n",
        "#y3_train, y3_val = y3[:split_size3], y3[split_size3:]\n",
        "#\n",
        "#train_sentences = np.concatenate((X1, X3)) \n",
        "#train_targets = np.concatenate((y1, y3))\n",
        "#train_contexts = []\n",
        "#\n",
        "#for line in train_sentences:\n",
        "##  train_contexts.append(\"\")\n",
        "#\n",
        "#\n",
        "#train_context_book = np.concatenate((train_context_book[500000:1000000], train_contexts)) \n",
        "#train_input_book = np.concatenate((train_input_book[500000:1000000], train_sentences)) \n",
        "#train_target_book = np.concatenate((train_target_book[500000:1000000], train_targets)) \n",
        "#val_sentences = np.concatenate((X1_val, X3_val))\n",
        "#val_targets = np.concatenate((y1_val, y3_val))"
      ],
      "metadata": {
        "id": "9MYX7qyLKGPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE= 64\n",
        "MAX_TOKENS = 256\n",
        "\n",
        "def filter_max_tokens(context, inp, tar):\n",
        "  num_tokens = tf.maximum(tf.shape(tar)[1], tf.shape(inp)[1])\n",
        "  num_tokens = tf.maximum(tf.shape(context)[1], num_tokens)\n",
        "\n",
        "  return num_tokens < MAX_TOKENS\n",
        "\n",
        "def tokenize_pairs(context, inp, tar): \n",
        "  context = tokenizer.tokenize(context)\n",
        "  inp = tokenizer.tokenize(inp)\n",
        "  tar = tokenizer.tokenize(tar)\n",
        "  return context.to_tensor(), inp.to_tensor(), tar.to_tensor()\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_context_book, train_input_book, train_target_book))\n",
        "#val_ds = tf.data.Dataset.from_tensor_slices((train_input_misc,train_context_misc, train_target_misc))\n",
        "\n",
        "train_ds = make_batches(train_ds)\n",
        "#val_ds = make_batches(val_ds)"
      ],
      "metadata": {
        "id": "fWg5FhurKGDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADn1RxPXfzYZ"
      },
      "source": [
        "# Position encoding and masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT2Dm2aI1tAK"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i , d_model):\n",
        "  angle_rate = 1/np.power(10000, 2*(i//2) / np.float32(d_model))\n",
        "  return pos/angle_rate\n",
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "  angle_rads = get_angles( np.arange(pos)[:, np.newaxis] , np.arange(d_model)[np.newaxis,:], d_model)\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "  pos_encdoing = angle_rads[np.newaxis, ...]\n",
        "  return tf.cast(pos_encdoing, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPpHqKQY1vDT"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size,size)), -1, 0)\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPqHf49q10Lt"
      },
      "source": [
        "# MultiHead Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7eoeg_019Q6"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lovBIx22ARj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5ApYdb84mg"
      },
      "source": [
        "# Encoder and Decoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3yeLpR487m3"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8297Zz6P88Hg"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLG3bkte9CQQ"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mhac = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernormc = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropoutc = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, context, training,\n",
        "           look_ahead_mask, padding_mask, padding_mask_context):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    attnc, attn_weights_blockc = self.mhac(\n",
        "        context, context, out2, padding_mask_context)  # (batch_size, target_seq_len, d_model)\n",
        "    attnc = self.dropoutc(attnc, training=training)\n",
        "    outc = self.layernorm2(attnc + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(outc)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + outc)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBCFBl1P9GXI"
      },
      "source": [
        "# Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH8hzlo49KxQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               rate=0.1, embedding_layer):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTmxgEBr9Neo"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               rate=0.1, embedding_layer):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, context, training,\n",
        "           look_ahead_mask, padding_mask, padding_mask_context):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    context = self.embedding(context)  # (batch_size, context_seq_length, d_model)\n",
        "    context *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    context += self.pos_encoding[:, :tf.shape(context)[1], :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, context, training,\n",
        "                                             look_ahead_mask, padding_mask, padding_mask_context)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqwr4Z6x9Rjw"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdG-glyH9TJ4"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           input_vocab_size=input_vocab_size, rate=rate, embedding_layer=EMBEDDING_LAYER)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           target_vocab_size=target_vocab_size, rate=rate, embedding_layer=EMBEDDING_LAYER)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs in the first argument\n",
        "    context, inp, tar = inputs\n",
        "\n",
        "    padding_mask, look_ahead_mask, padding_mask_context = self.create_masks(context, inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, context, training, look_ahead_mask, padding_mask, padding_mask_context)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, context, inp, tar):\n",
        "    # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
        "    padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return padding_mask, look_ahead_mask, create_padding_mask(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLpjcl8t9UwQ"
      },
      "source": [
        "# Hyperparameters and embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KavZudf9Wy4"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "EMBEDDING_LAYER = layers.Embedding(tokenizer.get_vocab_size().numpy() ,d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZDrBGM9je3"
      },
      "source": [
        "# Optimizer with custom learing rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymSFBABO9nRY"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps = 4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model \n",
        "    self.d_model = tf.cast(self.d_model, dtype= tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = step*1.35\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63JXagT29oG_"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9 ,beta_2 = 0.98, epsilon = 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY4DVjrR9uK5"
      },
      "source": [
        "# Loss and Metrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqL3ZFW49zsn"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbYytcQx933A"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FG6cV1y9_TP"
      },
      "source": [
        "# Training preprations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rfbhd9L-Omv"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HLehSR_-FLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed55d18-487b-4dfc-b3f0-b6ea71a3d6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latest checkpoint restored\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = 'drive/MyDrive/twct2wc_checkpoints/train'\n",
        "ckpt = tf.train.Checkpoint(transformer = transformer, optimizer = optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 5)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('latest checkpoint restored')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXKJvmvI-BCk"
      },
      "outputs": [],
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(context, inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([context, inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual training"
      ],
      "metadata": {
        "id": "xsuI6l2vhgZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fee1hK-m-enI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd94a94-4bde-4d3b-d2e8-4f6a2ce25a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.2134 Accuracy 0.9288\n",
            "Epoch 1 Batch 100 Loss 0.1946 Accuracy 0.9379\n",
            "Epoch 1 Batch 200 Loss 0.1893 Accuracy 0.9393\n",
            "Epoch 1 Batch 300 Loss 0.1896 Accuracy 0.9391\n",
            "Epoch 1 Batch 400 Loss 0.1893 Accuracy 0.9390\n",
            "Epoch 1 Batch 500 Loss 0.1904 Accuracy 0.9389\n",
            "Epoch 1 Batch 600 Loss 0.1923 Accuracy 0.9385\n",
            "Epoch 1 Batch 700 Loss 0.1920 Accuracy 0.9388\n",
            "Epoch 1 Batch 800 Loss 0.1926 Accuracy 0.9387\n",
            "Epoch 1 Batch 900 Loss 0.1920 Accuracy 0.9389\n",
            "Epoch 1 Batch 1000 Loss 0.1910 Accuracy 0.9392\n",
            "Epoch 1 Batch 1100 Loss 0.1902 Accuracy 0.9394\n",
            "Epoch 1 Batch 1200 Loss 0.1896 Accuracy 0.9396\n",
            "Epoch 1 Batch 1300 Loss 0.1906 Accuracy 0.9394\n",
            "Epoch 1 Batch 1400 Loss 0.1931 Accuracy 0.9387\n",
            "Epoch 1 Batch 1500 Loss 0.1937 Accuracy 0.9386\n",
            "Epoch 1 Batch 1600 Loss 0.1949 Accuracy 0.9384\n",
            "Epoch 1 Batch 1700 Loss 0.1975 Accuracy 0.9377\n",
            "Epoch 1 Batch 1800 Loss 0.1990 Accuracy 0.9372\n",
            "Epoch 1 Batch 1900 Loss 0.2004 Accuracy 0.9367\n",
            "Epoch 1 Batch 2000 Loss 0.2013 Accuracy 0.9363\n",
            "Epoch 1 Batch 2100 Loss 0.2030 Accuracy 0.9358\n",
            "Epoch 1 Batch 2200 Loss 0.2043 Accuracy 0.9354\n",
            "Epoch 1 Batch 2300 Loss 0.2053 Accuracy 0.9352\n",
            "Epoch 1 Batch 2400 Loss 0.2055 Accuracy 0.9354\n",
            "Epoch 1 Batch 2500 Loss 0.2044 Accuracy 0.9357\n",
            "Epoch 1 Batch 2600 Loss 0.2025 Accuracy 0.9363\n",
            "Epoch 1 Batch 2700 Loss 0.2014 Accuracy 0.9366\n",
            "Epoch 1 Batch 2800 Loss 0.2012 Accuracy 0.9367\n",
            "Epoch 1 Batch 2900 Loss 0.2016 Accuracy 0.9366\n",
            "Epoch 1 Batch 3000 Loss 0.2022 Accuracy 0.9364\n",
            "Epoch 1 Batch 3100 Loss 0.2026 Accuracy 0.9362\n",
            "Epoch 1 Batch 3200 Loss 0.2031 Accuracy 0.9361\n",
            "Epoch 1 Batch 3300 Loss 0.2031 Accuracy 0.9361\n",
            "Epoch 1 Batch 3400 Loss 0.2034 Accuracy 0.9360\n",
            "Epoch 1 Batch 3500 Loss 0.2042 Accuracy 0.9358\n",
            "Epoch 1 Batch 3600 Loss 0.2053 Accuracy 0.9355\n",
            "Epoch 1 Batch 3700 Loss 0.2051 Accuracy 0.9356\n",
            "Epoch 1 Batch 3800 Loss 0.2044 Accuracy 0.9358\n",
            "Epoch 1 Batch 3900 Loss 0.2038 Accuracy 0.9360\n",
            "Epoch 1 Batch 4000 Loss 0.2035 Accuracy 0.9360\n",
            "Epoch 1 Batch 4100 Loss 0.2035 Accuracy 0.9361\n",
            "Epoch 1 Batch 4200 Loss 0.2036 Accuracy 0.9360\n",
            "Epoch 1 Batch 4300 Loss 0.2034 Accuracy 0.9361\n",
            "Epoch 1 Batch 4400 Loss 0.2034 Accuracy 0.9361\n",
            "Epoch 1 Batch 4500 Loss 0.2032 Accuracy 0.9361\n",
            "Epoch 1 Batch 4600 Loss 0.2029 Accuracy 0.9362\n",
            "Epoch 1 Batch 4700 Loss 0.2027 Accuracy 0.9363\n",
            "Epoch 1 Batch 4800 Loss 0.2023 Accuracy 0.9364\n",
            "Epoch 1 Batch 4900 Loss 0.2019 Accuracy 0.9365\n",
            "Epoch 1 Batch 5000 Loss 0.2019 Accuracy 0.9365\n",
            "Epoch 1 Batch 5100 Loss 0.2020 Accuracy 0.9365\n",
            "Epoch 1 Batch 5200 Loss 0.2019 Accuracy 0.9364\n",
            "Epoch 1 Batch 5300 Loss 0.2022 Accuracy 0.9363\n",
            "Epoch 1 Batch 5400 Loss 0.2022 Accuracy 0.9364\n",
            "Epoch 1 Batch 5500 Loss 0.2017 Accuracy 0.9365\n",
            "Epoch 1 Batch 5600 Loss 0.2015 Accuracy 0.9366\n",
            "Epoch 1 Batch 5700 Loss 0.2016 Accuracy 0.9365\n",
            "Epoch 1 Batch 5800 Loss 0.2020 Accuracy 0.9364\n",
            "Epoch 1 Batch 5900 Loss 0.2023 Accuracy 0.9363\n",
            "Epoch 1 Batch 6000 Loss 0.2026 Accuracy 0.9362\n",
            "Epoch 1 Batch 6100 Loss 0.2025 Accuracy 0.9362\n",
            "Epoch 1 Batch 6200 Loss 0.2021 Accuracy 0.9364\n",
            "Epoch 1 Batch 6300 Loss 0.2017 Accuracy 0.9365\n",
            "Epoch 1 Batch 6400 Loss 0.2010 Accuracy 0.9367\n",
            "Epoch 1 Batch 6500 Loss 0.2004 Accuracy 0.9369\n",
            "Epoch 1 Batch 6600 Loss 0.2007 Accuracy 0.9369\n",
            "Epoch 1 Batch 6700 Loss 0.2009 Accuracy 0.9368\n",
            "Epoch 1 Batch 6800 Loss 0.2010 Accuracy 0.9368\n",
            "Epoch 1 Batch 6900 Loss 0.2012 Accuracy 0.9367\n",
            "Epoch 1 Batch 7000 Loss 0.2015 Accuracy 0.9366\n",
            "Epoch 1 Batch 7100 Loss 0.2016 Accuracy 0.9366\n",
            "Epoch 1 Batch 7200 Loss 0.2015 Accuracy 0.9366\n",
            "Epoch 1 Batch 7300 Loss 0.2015 Accuracy 0.9366\n",
            "Epoch 1 Batch 7400 Loss 0.2013 Accuracy 0.9367\n",
            "Epoch 1 Batch 7500 Loss 0.2010 Accuracy 0.9368\n",
            "Epoch 1 Batch 7600 Loss 0.2007 Accuracy 0.9369\n",
            "Epoch 1 Batch 7700 Loss 0.2006 Accuracy 0.9369\n",
            "Epoch 1 Batch 7800 Loss 0.2006 Accuracy 0.9369\n",
            "Epoch 1 Batch 7900 Loss 0.2005 Accuracy 0.9370\n",
            "Epoch 1 Batch 8000 Loss 0.2005 Accuracy 0.9370\n",
            "Epoch 1 Batch 8100 Loss 0.2006 Accuracy 0.9369\n",
            "Epoch 1 Batch 8200 Loss 0.2008 Accuracy 0.9369\n",
            "Epoch 1 Batch 8300 Loss 0.2009 Accuracy 0.9369\n",
            "Epoch 1 Batch 8400 Loss 0.2014 Accuracy 0.9367\n",
            "Epoch 1 Batch 8500 Loss 0.2021 Accuracy 0.9365\n",
            "Epoch 1 Batch 8600 Loss 0.2026 Accuracy 0.9364\n",
            "Epoch 1 Batch 8700 Loss 0.2028 Accuracy 0.9363\n",
            "Epoch 1 Batch 8800 Loss 0.2029 Accuracy 0.9363\n",
            "Epoch 1 Batch 8900 Loss 0.2027 Accuracy 0.9363\n",
            "Epoch 1 Batch 9000 Loss 0.2026 Accuracy 0.9364\n",
            "Epoch 1 Batch 9100 Loss 0.2029 Accuracy 0.9363\n",
            "Epoch 1 Batch 9200 Loss 0.2029 Accuracy 0.9363\n",
            "Epoch 1 Batch 9300 Loss 0.2028 Accuracy 0.9364\n",
            "Epoch 1 Batch 9400 Loss 0.2027 Accuracy 0.9364\n",
            "Epoch 1 Batch 9500 Loss 0.2026 Accuracy 0.9364\n",
            "Epoch 1 Batch 9600 Loss 0.2021 Accuracy 0.9366\n",
            "Epoch 1 Batch 9700 Loss 0.2017 Accuracy 0.9367\n",
            "Epoch 1 Batch 9800 Loss 0.2011 Accuracy 0.9369\n",
            "Epoch 1 Batch 9900 Loss 0.2005 Accuracy 0.9371\n",
            "Epoch 1 Batch 10000 Loss 0.2003 Accuracy 0.9371\n",
            "Epoch 1 Batch 10100 Loss 0.2003 Accuracy 0.9371\n",
            "Epoch 1 Batch 10200 Loss 0.2003 Accuracy 0.9371\n",
            "Epoch 1 Batch 10300 Loss 0.2002 Accuracy 0.9372\n",
            "Epoch 1 Batch 10400 Loss 0.2005 Accuracy 0.9371\n",
            "Epoch 1 Batch 10500 Loss 0.2006 Accuracy 0.9371\n",
            "Epoch 1 Batch 10600 Loss 0.2008 Accuracy 0.9370\n",
            "Epoch 1 Batch 10700 Loss 0.2007 Accuracy 0.9370\n",
            "Epoch 1 Batch 10800 Loss 0.2004 Accuracy 0.9371\n",
            "Epoch 1 Batch 10900 Loss 0.2003 Accuracy 0.9372\n",
            "Epoch 1 Batch 11000 Loss 0.2003 Accuracy 0.9372\n",
            "Epoch 1 Batch 11100 Loss 0.2002 Accuracy 0.9372\n",
            "Epoch 1 Batch 11200 Loss 0.1997 Accuracy 0.9373\n",
            "Epoch 1 Batch 11300 Loss 0.1996 Accuracy 0.9374\n",
            "Saving checkpoint for epoch 1 at drive/MyDrive/twct2wc_checkpoints/train/ckpt-11\n",
            "Epoch 1 Loss 0.1995 Accuracy 0.9374\n",
            "Time taken for 1 epoch: 5531.66 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (context, inp, tar)) in enumerate(train_ds):\n",
        "    train_step(context, inp, tar)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 1 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTWwtV7-vFq"
      },
      "source": [
        "# Getting output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYpQWt_WBmj2"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizer, transformer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizer.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    start_end = self.tokenizer.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions, _ = self.transformer([tokenizer.tokenize([\"I am fine\"]).to_tensor(), encoder_input, output], training=False)\n",
        "\n",
        "      # select the last token from the seq_len dimension\n",
        "      predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # concatentate the predicted_id to the output which is given to the decoder\n",
        "      # as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # output.shape (1, tokens)\n",
        "    text = tokenizer.detokenize(output)[0]  # shape: ()\n",
        "\n",
        "    tokens = tokenizer.lookup(output)[0]\n",
        "\n",
        "    _, attention_weights = self.transformer([tokenizer.tokenize([\"I am fine\"]).to_tensor(), encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eexdpKK4CaSm"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizer, transformer)\n",
        "\n",
        "def print_translation(sentence, tokens,):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'can I go park mom'\n",
        "translated_text, _, _  = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text)"
      ],
      "metadata": {
        "id": "R22SCKVCEHy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aed1bc3-6448-44e7-fd1c-fc80a024ea78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : can I go park mom\n",
            "Prediction     : can i go to the park of mom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "AhEgRLfOUvMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save_weights(\"/drive/MyDrive/twct2weights\")\n",
        "transformer.save(\"/drive/MyDrive/txct2\")"
      ],
      "metadata": {
        "id": "ZpgjME4pUxU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ly4cTJHGwF0J",
        "k9IMXTydglzK",
        "DpmupdURcW-7",
        "_hGwYOEHcyxB",
        "ADn1RxPXfzYZ",
        "hPqHf49q10Lt",
        "-u5ApYdb84mg",
        "qBCFBl1P9GXI",
        "fqwr4Z6x9Rjw",
        "YLpjcl8t9UwQ",
        "RFZDrBGM9je3",
        "jY4DVjrR9uK5",
        "8FG6cV1y9_TP",
        "QFTWwtV7-vFq"
      ],
      "machine_shape": "hm",
      "name": "trasnformer_with_context_take2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}